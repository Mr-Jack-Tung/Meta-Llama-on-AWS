{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7448c728-bd09-49e3-9223-6fe4605679e8",
   "metadata": {},
   "source": [
    "# Getting started with Llama 3 on AWS\n",
    "\n",
    "## Llama 3\n",
    "\n",
    "Llama 3 (Large Language Model Meta AI) is the third iteration of Meta's advanced language models, designed for tasks like text generation, translation, and summarization. Built on transformer architecture, it excels at understanding and generating human-like text by training on extensive datasets from diverse sources.\n",
    "\n",
    "## Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.\n",
    "\n",
    "AWS Bedrock can be used to access and deploy LLaMA 3 by providing the necessary infrastructure and tools. Developers can integrate LLaMA 3 into their applications through Bedrock's APIs, customize it with specific datasets, and scale the deployment as needed, leveraging AWS's robust infrastructure and cost management features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f467c8b-bcd7-4f5b-867c-6230ca6ccd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a couple utility functions. You can skip this section.\n",
    "\n",
    "import rich\n",
    "\n",
    "def print_json(data):\n",
    "    rich.print_json(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6638c-1c46-479b-a0f9-a443fb9156d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Large language models produce non-deterministic results, you may see different outputs than those presented in this notebook. Running this code from your own AWS account will incur charges for the tokens used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17e1c0-330a-4ecc-b8c0-920b1f6c4533",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> This notebook is vetted to run on a [SageMaker Studio](https://aws.amazon.com/sagemaker/studio/) Jupyter notebook running the `ipykernel`. Also, the credentials, namely AWS Access Key and AWS Secret Access key, are assigned as an IAM Role to the notebook instance, hence why they are not hard-coded anywhere in this code. If you run this outside of SageMaker Studio, make the right ajustement to [authenticate your requests](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) to the Bedrock API with your AWS Access Key.\n",
    "</div>\n",
    "\n",
    "[Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) is a Python library that allows you to interact with AWS resources programmatically. It provides an easy way to automate tasks and manage AWS services through code. We'll use Boto3 to make requests and retrieve data from the Amazon Bedrock API. The Boto3 Bedrock SDK includes four clients designed to interact with different aspects of Bedrock:\n",
    "\n",
    "- **bedrock**: Includes APIs for controlling model management, training, and deployment.\n",
    "- **bedrock-runtime**: Includes APIs for making inference requests to models hosted in Amazon Bedrock.\n",
    "- **bedrock-agent**: Provides APIs for creating and managing agents and knowledge bases.\n",
    "- **bedrock-agent-runtime**: Includes APIs for controlling model management, training, and deployment for agents and knowledge bases.\n",
    "\n",
    "Let's start by installing the latest version of boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9f4ba1-1d2a-4fbb-a7e4-f2a3e2e79960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest version of boto3\n",
    "!python3 -m pip install --quiet --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b14fc0f-48ff-402d-b026-e1b280b6fe15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.138\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce16074-2b55-4816-8442-e7b005d7b411",
   "metadata": {},
   "source": [
    "To kick things off, we list all models available via Bedrock from Meta. Note that differents models will be available based on the AWS Region you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c46e91-9c58-4bd6-aa8d-b4fbb62184db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3b3f1555-674a-4f08-8a51-d21f184a015c',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Wed, 03 Jul 2024 18:06:17 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '3672',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '3b3f1555-674a-4f08-8a51-d21f184a015c'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'LEGACY'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set default AWS region\n",
    "default_region = \"us-east-1\"\n",
    "\n",
    "# Create a Bedrock client in the AWS Region of your choice.\n",
    "bedrock = boto3.client(\"bedrock\", region_name=default_region)\n",
    "\n",
    "# List all models from meta\n",
    "models = bedrock.list_foundation_models(\n",
    "    byProvider='Meta' # comment this line to get all models from all providers\n",
    ")\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c72108-fadc-47bb-a5cf-81eec3c1ff1c",
   "metadata": {},
   "source": [
    "The output returns many important attributes for each models:\n",
    "\n",
    "| Field                        | Description                                                              |\n",
    "|------------------------------|--------------------------------------------------------------------------|\n",
    "| `modelArn`                   | ARN that uniquely identifies the model in AWS Bedrock.                   |\n",
    "| `modelId`                    | Unique identifier for the model within AWS Bedrock.                      |\n",
    "| `modelName`                  | Name or title of the model.                                              |\n",
    "| `providerName`               | Organization or entity providing the model.                              |\n",
    "| `inputModalities`            | Types of inputs the model accepts (e.g., `'TEXT'`).                        |\n",
    "| `outputModalities`           | Types of outputs the model generates (e.g., `'TEXT'`).                     |\n",
    "| `responseStreamingSupported` | Indicates if the model supports streaming responses.                     |\n",
    "| `customizationsSupported`    | Lists any customization options available for the model.                 |\n",
    "| `inferenceTypesSupported`    | Describes the ways inference can be requested (e.g., `'ON_DEMAND'`).       |\n",
    "| `modelLifecycle`             | Current status of the model (e.g., `'ACTIVE'`).                            |\n",
    "\n",
    "\n",
    "Another way to list all models in a more readable fashion is as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19701829-2121-4dca-b97f-24c659cd372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta.llama2-13b-chat-v1:0:4k\n",
      "meta.llama2-13b-chat-v1\n",
      "meta.llama2-70b-chat-v1:0:4k\n",
      "meta.llama2-70b-chat-v1\n",
      "meta.llama2-13b-v1:0:4k\n",
      "meta.llama2-13b-v1\n",
      "meta.llama2-70b-v1:0:4k\n",
      "meta.llama2-70b-v1\n",
      "meta.llama3-8b-instruct-v1:0\n",
      "meta.llama3-70b-instruct-v1:0\n"
     ]
    }
   ],
   "source": [
    "for model in models['modelSummaries']:\n",
    "    print(model['modelId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df208a1-e7ce-4883-a24d-2219113a71c2",
   "metadata": {},
   "source": [
    "## Calling a model\n",
    "\n",
    "The first example consist of a call to the Bedrock API to pass a prompt and receive an answer from the LLM.\n",
    "\n",
    "The `InvokeModel` API call the specified Amazon Bedrock model to run inference using the prompt and inference parameters provided in the request body. Depending on the model, you can infer text, images or embeddings.\n",
    "\n",
    "API documentation: https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db223e27-05bd-4e0d-bac4-8a0fb8aa17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of a \"Hello World\" program is to serve as a simple, introductory example of a working program in a programming language, demonstrating the basic syntax and structure of the language, and providing a starting point for new programmers to learn and experiment with.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set the model ID.\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Set the prompt.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=default_region)\n",
    "\n",
    "# Embed the prompt in Llama 3's instruction format.\n",
    "# More information: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "    \n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"generation\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c2dc1-751d-4ad7-8b80-b8dd78cd4c34",
   "metadata": {},
   "source": [
    "Additionally, Llama 2 Chat, Llama 2, and Llama 3 Instruct models return the following fields for a text completion inference call alongside the generated text by the model.\n",
    " \n",
    "| Field                    | Description                                                                                                                                                                           |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `generation`             | The generated text.                                                                                                                                                                   |\n",
    "| `prompt_token_count`     | The number of tokens in the prompt.                                                                                                                                                   |\n",
    "| `generation_token_count` | The number of tokens in the generated text.                                                                                                                                           |\n",
    "| `stop_reason`            | The reason why the response stopped generating text. Possible values are: <br> - `stop`: The model has finished generating text for the input prompt. <br> - `length`: The length of the tokens for the generated text exceeds the value of `max_gen_len` in the call to `InvokeModel` (`InvokeModelWithResponseStream`, if you are streaming output). The response is truncated to `max_gen_len` tokens. Consider increasing the value of `max_gen_len` and trying again. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60829de-934b-43d4-ba70-9afdead49cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"The purpose of a \\\"Hello World\\\" program is to serve as a simple, introductory example of a working program in a programming language, demonstrating the basic syntax and structure of the language, and providing a starting point for new programmers to learn and experiment with.\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"prompt_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"generation_token_count\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"stop_reason\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"stop\"</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"generation\"\u001b[0m: \u001b[32m\"The purpose of a \\\"Hello World\\\" program is to serve as a simple, introductory example of a working program in a programming language, demonstrating the basic syntax and structure of the language, and providing a starting point for new programmers to learn and experiment with.\"\u001b[0m,\n",
       "  \u001b[1;34m\"prompt_token_count\"\u001b[0m: \u001b[1;36m27\u001b[0m,\n",
       "  \u001b[1;34m\"generation_token_count\"\u001b[0m: \u001b[1;36m52\u001b[0m,\n",
       "  \u001b[1;34m\"stop_reason\"\u001b[0m: \u001b[32m\"stop\"\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_json(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98766258-1ba5-45aa-a0e1-205ff67532ba",
   "metadata": {},
   "source": [
    "The drawback of using the InvokeModel API lies in its requirement for different JSON request and response structures depending on the model provider. Recall the following code snippet from the example:\n",
    "\n",
    "\n",
    "```python\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Switching from llama2 or llama3 to another model with a different prompt structure, such as from a different provider (or maybe even a future release of llama), would necessitate rewriting the code. This situation leads to managing diverse formats, complicating integration efforts.\n",
    "\n",
    "A better approach is to use the Amazon Bedrock `Converse` API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef9132-eac2-4d63-b9e6-3d18c0b0d706",
   "metadata": {},
   "source": [
    "### Bedrock converse API\n",
    "\n",
    "The [Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html) is designed for creating advanced conversational applications by interacting with large language models like Llama3. It allows developers to send conversation prompts and receive contextually relevant responses, maintaining dialogue coherence over multiple exchanges.\n",
    "\n",
    "Compared to the InvokeModel API, the Converse API offers advantages in dialogue management and context retention. While `InvokeModel` handles single, standalone prompts, the `Converse` API is built to maintain the context of an ongoing conversation, making it more suitable for applications that require multi-turn interactions and a natural flow of dialogue. This enhanced capability results in more engaging and effective conversational agents.\n",
    "\n",
    "For a complete guide, see [Getting started with the Amazon Bedrock Converse API\n",
    "](https://community.aws/content/2hHgVE7Lz6Jj1vFv39zSzzlCilG/getting-started-with-the-amazon-bedrock-converse-api?lang=en)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f12198-b9ee-47df-aa10-f15a367e5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A \"Hello World\" program is a simple computer program that prints or displays the text \"Hello, World!\" to demonstrate the basic syntax and functionality of a programming language or development environment.\n"
     ]
    }
   ],
   "source": [
    "# Use the Conversation API to send a text message to Meta Llama.\n",
    "\n",
    "def send_message_to_model(conversation, model_id=model_id, max_tokens=512, temperature=0.5, top_p=0.9, system_prompt=\"You are a helpful assistant\"):\n",
    "    try:\n",
    "        # Send the message to the model, using the provided inference configuration.\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\"maxTokens\": max_tokens, \"temperature\": temperature, \"topP\": top_p},\n",
    "            system=[{\"text\":system_prompt}],\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "        return response\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = send_message_to_model(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64866076-e272-4bb2-ba23-90aaded35cb4",
   "metadata": {},
   "source": [
    "Alternatively, we can print the whole conversation. Notice the two roles `user` and `assistant` alterning between each other. The last message in the list should be from the `user` role, so that the LLM can respond to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f5cca4-49d2-4d95-b40a-ad3349695643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"user\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "      <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Describe the purpose of a 'hello world' program in one line.\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"assistant\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "      <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\n\\nA \\\"Hello World\\\" program is a simple computer program that prints or displays the text \\\"Hello, World!\\\" to demonstrate the basic syntax and functionality of a programming language or development environment.\"</span>\n",
       "      <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"user\"\u001b[0m,\n",
       "    \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "      \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"Describe the purpose of a 'hello world' program in one line.\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"assistant\"\u001b[0m,\n",
       "    \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "      \u001b[1m{\u001b[0m\n",
       "        \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"\\n\\nA \\\"Hello World\\\" program is a simple computer program that prints or displays the text \\\"Hello, World!\\\" to demonstrate the basic syntax and functionality of a programming language or development environment.\"\u001b[0m\n",
       "      \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.append(response[\"output\"][\"message\"])\n",
    "print_json(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a69d97-68c5-408d-be67-f9b866808de8",
   "metadata": {},
   "source": [
    "#### Setting a system prompt\n",
    "\n",
    "You can set a system prompt to communicate basic instructions for the large language model outside of the normal conversation. System prompts are generally used by the developer to define the tone and constraints for the conversation. In this case, we’re instructing Llama to act like a pirate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0511d3a-85b9-499e-9ae8-049c8fce22e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrr, shiver me timbers! The best place to hide a pirate booty be a place that's hard to find, but not impossible. I'd say, stash yer loot on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs that live there. Make sure it be hidden good, with traps and puzzles to keep landlubbers from gettin' their grubby hands on it. And don't ferget to leave a treasure map, just in case ye need to find it yerself!\n"
     ]
    }
   ],
   "source": [
    "new_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        { \"text\": \"What is the best place to hide a pirate booty?\" } \n",
    "    ],\n",
    "}\n",
    "\n",
    "system_prompt=\"Answer in the style of a pirate\"\n",
    "\n",
    "conversation.append(new_message)\n",
    "response = send_message_to_model(conversation, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970fbca-cdc2-4b1c-8682-4e97ec2aaa77",
   "metadata": {},
   "source": [
    "#### Getting response metadata and token counts\n",
    "\n",
    "The Converse method also returns metadata about the API call. The `stopReason` property tells us why the model completed the message. This can be useful for your application logic, error handling, or troubleshooting. The `usage` property includes details about the input and output tokens. This can help you understand the charges for your API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea801886-834a-4c35-b234-1dc0a56ad1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"ResponseMetadata\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RequestId\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"825172f2-6040-4346-83f7-8fc21a478d63\"</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPStatusCode\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"HTTPHeaders\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"date\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"Wed, 03 Jul 2024 18:06:21 GMT\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-type\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"application/json\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content-length\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"637\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"connection\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"keep-alive\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"x-amzn-requestid\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"825172f2-6040-4346-83f7-8fc21a478d63\"</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"RetryAttempts\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"output\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"message\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"role\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"assistant\"</span>,\n",
       "      <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"content\"</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "          <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"text\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"\\n\\nArrr, shiver me timbers! The best place to hide a pirate booty be a place that's hard to find, but not impossible. I'd say, stash yer loot on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs that live there. Make sure it be hidden good, with traps and puzzles to keep landlubbers from gettin' their grubby hands on it. And don't ferget to leave a treasure map, just in case ye need to find it yerself!\"</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "      <span style=\"font-weight: bold\">]</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"stopReason\"</span>: <span style=\"color: #008000; text-decoration-color: #008000\">\"end_turn\"</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"usage\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"inputTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">93</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"outputTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">118</span>,\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"totalTokens\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">211</span>\n",
       "  <span style=\"font-weight: bold\">}</span>,\n",
       "  <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"metrics\"</span>: <span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">\"latencyMs\"</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1715</span>\n",
       "  <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "  \u001b[1;34m\"ResponseMetadata\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"RequestId\"\u001b[0m: \u001b[32m\"825172f2-6040-4346-83f7-8fc21a478d63\"\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPStatusCode\"\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "    \u001b[1;34m\"HTTPHeaders\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"date\"\u001b[0m: \u001b[32m\"Wed, 03 Jul 2024 18:06:21 GMT\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-type\"\u001b[0m: \u001b[32m\"application/json\"\u001b[0m,\n",
       "      \u001b[1;34m\"content-length\"\u001b[0m: \u001b[32m\"637\"\u001b[0m,\n",
       "      \u001b[1;34m\"connection\"\u001b[0m: \u001b[32m\"keep-alive\"\u001b[0m,\n",
       "      \u001b[1;34m\"x-amzn-requestid\"\u001b[0m: \u001b[32m\"825172f2-6040-4346-83f7-8fc21a478d63\"\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[1;34m\"RetryAttempts\"\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"output\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"message\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "      \u001b[1;34m\"role\"\u001b[0m: \u001b[32m\"assistant\"\u001b[0m,\n",
       "      \u001b[1;34m\"content\"\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\n",
       "          \u001b[1;34m\"text\"\u001b[0m: \u001b[32m\"\\n\\nArrr, shiver me timbers! The best place to hide a pirate booty be a place that's hard to find, but not impossible. I'd say, stash yer loot on a deserted isle, deep in the jungle, where the only creatures that'll find it be the scurvy dogs that live there. Make sure it be hidden good, with traps and puzzles to keep landlubbers from gettin' their grubby hands on it. And don't ferget to leave a treasure map, just in case ye need to find it yerself!\"\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "      \u001b[1m]\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"stopReason\"\u001b[0m: \u001b[32m\"end_turn\"\u001b[0m,\n",
       "  \u001b[1;34m\"usage\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"inputTokens\"\u001b[0m: \u001b[1;36m93\u001b[0m,\n",
       "    \u001b[1;34m\"outputTokens\"\u001b[0m: \u001b[1;36m118\u001b[0m,\n",
       "    \u001b[1;34m\"totalTokens\"\u001b[0m: \u001b[1;36m211\u001b[0m\n",
       "  \u001b[1m}\u001b[0m,\n",
       "  \u001b[1;34m\"metrics\"\u001b[0m: \u001b[1m{\u001b[0m\n",
       "    \u001b[1;34m\"latencyMs\"\u001b[0m: \u001b[1;36m1715\u001b[0m\n",
       "  \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_json(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b6b7a9-b33c-45e8-b85b-a8123c725d4a",
   "metadata": {},
   "source": [
    "## Additional Ressource\n",
    "\n",
    "- Meta's Llama receipt for AWS: https://github.com/meta-llama/llama-recipes/tree/main/recipes/3p_integrations/aws\n",
    "- Amazon Bedrock samples: https://github.com/aws-samples/amazon-bedrock-samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
