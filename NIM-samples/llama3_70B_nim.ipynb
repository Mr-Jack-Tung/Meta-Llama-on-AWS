{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy Llama 3 70B with NVIDIA NIM on Amazon SageMaker\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "## What is NIM\n",
    "\n",
    "NVIDIA NIM enables efficient deployment of large language models (LLMs) across various environments, including cloud, data centers, and workstations. It simplifies self-hosting LLMs by providing scalable, high-performance microservices optimized for NVIDIA GPUs. NIM's containerized approach allows for easy integration into existing workflows, with support for advanced language models and enterprise-grade security. Leveraging GPU acceleration, NIM offers fast inference capabilities and flexible deployment options, empowering developers to build powerful AI applications such as chatbots, content generators, and translation services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b92143-a092-4799-a492-8e0ee0908176",
   "metadata": {},
   "source": [
    "### Features\n",
    "\n",
    "NIM abstracts away model inference internals such as execution engine and runtime operations. They are also the most performant option available whether it be with TRT-LLM, vLLM or others. NIM offers the following high performance features:\n",
    "\n",
    "1. Scalable Deployment that is performant and can easily and seamlessly scale from a few users to millions.\n",
    "2. Advanced Language Model support with pre-generated optimized engines for a diverse range of cutting edge LLM architectures.\n",
    "3. Flexible Integration to easily incorporate the microservice into existing workflows and applications. Developers are provided with an OpenAI API compatible programming model and custom NVIDIA extensions for additional functionality.\n",
    "4. Enterprise-Grade Security emphasizes security by using safetensors, constantly monitoring and patching CVEs in our stack and conducting internal penetration tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007649f-5837-4575-a7a9-a2b6c730f5a0",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "NIMs are packaged as container images on a per model/model family basis. Each NIM is its own Docker container with a model, such as llama3. These containers include a runtime that runs on any NVIDIA GPU with sufficient GPU memory, but some model/GPU combinations are optimized. NIMs are distributed as NGC container images through the NVIDIA NGC Catalog. NIM automatically downloads the model from NGC, leveraging a local filesystem cache if available. Each NIM is built from a common base, so once a NIM has been downloaded, downloading additional NIMs is extremely fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `Llama3 70B` on a `p4d.24xlarge` instance with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d95e7-4bcc-4c83-90bd-1c492c67aa24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Card\n",
    "---\n",
    "### Llama 3 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. \n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935d62c-0675-4a62-9e46-d9e854491a26",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599941-1c76-4352-b1c3-eca6f4a65aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b>  To run NIM on SageMaker you will need to have your `NGC API KEY` to access NGC resources. Check out <a href=\"https://build.nvidia.com/meta/llama3-70b?signin=true\"> this LINK</a> to learn how to get an NGC API KEY. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd7912-b1cb-478f-b344-ceb2f0bfb770",
   "metadata": {},
   "source": [
    "##### 1. Setup and retrieve API key:\n",
    "\n",
    "1. First you will need to sign into [NGC](9https://ngc.nvidia.com/signin) with your NVIDIA account and password.\n",
    "2. Navigate to setup.\n",
    "3. Select “Get API Key”.\n",
    "4. Generate your API key.\n",
    "5. Keep your API key secret and in a safe place. Do not share it or store it in a place where others can see or copy it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "For more information on NIM, check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb4d59-c7ec-45de-9018-8c05052625ce",
   "metadata": {},
   "source": [
    "##### 2. You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ea72c-6da0-4236-8975-f70b616ceaa2",
   "metadata": {},
   "source": [
    "##### 3. NIM public ECR image is currently available only in `us-east-1` region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acad648-c837-40bc-b2e3-94016333ea2b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3 \n",
    "import json\n",
    "import os\n",
    "import sagemaker\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7acbd-edad-4e3e-9386-1e587879b2a5",
   "metadata": {},
   "source": [
    "### Set Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee757",
   "metadata": {},
   "source": [
    "In this example, since we are deploying `Llama3 70B` we define some configurations below for retrieving our ECR image for NIM along with some other requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f64791-1c45-4b84-9b1a-1e3ebc60d2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# llama-3-70b\n",
    "public_nim_image = \"public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\"\n",
    "nim_model = \"nim-llama3-70b-instruct\"\n",
    "sm_model_name = \"nim-llama3-70b-instruct\"\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "payload_model = \"meta/llama3-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05462",
   "metadata": {},
   "source": [
    "### NIM Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851abe8-9ca4-403b-be7f-aef56dfa4b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first pull the NIM image from public ECR and then push it to private ECR repo within your account for deploying on SageMaker endpoint. \n",
    "\n",
    "Note, as mentioned previously:\n",
    "  - NIM ECR image is currently available only in `us-east-1` region\n",
    "  - You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944110e5-15bc-4a94-a731-7d4ea9344e9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS account ID: 570598552974\n",
      "Public NIM Image: public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\n",
      "llama3-70b-instruct-1.0.0: Pulling from nvidia/nim\n",
      "cbe3537751ce: Already exists\n",
      "d67fcc6ef577: Already exists\n",
      "47ee674c5713: Already exists\n",
      "63daa0e64b30: Already exists\n",
      "d9d9aecefab5: Already exists\n",
      "d71f46a15657: Pulling fs layer\n",
      "054e2ffff644: Pulling fs layer\n",
      "7d3cd81654d5: Pulling fs layer\n",
      "dca613dca886: Pulling fs layer\n",
      "0fdcdcda3b2e: Pulling fs layer\n",
      "af7b4f7dc15a: Pulling fs layer\n",
      "6d101782f66c: Pulling fs layer\n",
      "e8427cb13897: Pulling fs layer\n",
      "de05b029a5a2: Pulling fs layer\n",
      "3d72a2698104: Pulling fs layer\n",
      "aeff973c2191: Pulling fs layer\n",
      "85d7d3ff0cca: Pulling fs layer\n",
      "5996430251dd: Pulling fs layer\n",
      "314dc83fdfc2: Pulling fs layer\n",
      "5cef8f59ae9a: Pulling fs layer\n",
      "927db4ce3e96: Pulling fs layer\n",
      "cbe4a04f4491: Pulling fs layer\n",
      "e8427cb13897: Waiting\n",
      "ddb02ef514f8: Pulling fs layer\n",
      "6d101782f66c: Waiting\n",
      "de05b029a5a2: Waiting\n",
      "a76bf8c63bc7: Pulling fs layer\n",
      "4aadc1bd8c8a: Pulling fs layer\n",
      "3d72a2698104: Waiting\n",
      "314dc83fdfc2: Waiting\n",
      "268bcca40b76: Pulling fs layer\n",
      "aeff973c2191: Waiting\n",
      "83c9f4bab802: Pulling fs layer\n",
      "0a4990a32bb5: Pulling fs layer\n",
      "927db4ce3e96: Waiting\n",
      "cbe4a04f4491: Waiting\n",
      "ddb02ef514f8: Waiting\n",
      "5cef8f59ae9a: Waiting\n",
      "85d7d3ff0cca: Waiting\n",
      "a76bf8c63bc7: Waiting\n",
      "5996430251dd: Waiting\n",
      "0fdcdcda3b2e: Waiting\n",
      "dca613dca886: Waiting\n",
      "af7b4f7dc15a: Waiting\n",
      "268bcca40b76: Waiting\n",
      "0a4990a32bb5: Waiting\n",
      "83c9f4bab802: Waiting\n",
      "d71f46a15657: Verifying Checksum\n",
      "d71f46a15657: Download complete\n",
      "d71f46a15657: Pull complete\n",
      "dca613dca886: Verifying Checksum\n",
      "dca613dca886: Download complete\n",
      "0fdcdcda3b2e: Verifying Checksum\n",
      "0fdcdcda3b2e: Download complete\n",
      "054e2ffff644: Download complete\n",
      "6d101782f66c: Verifying Checksum\n",
      "6d101782f66c: Download complete\n",
      "e8427cb13897: Verifying Checksum\n",
      "e8427cb13897: Download complete\n",
      "de05b029a5a2: Verifying Checksum\n",
      "de05b029a5a2: Download complete\n",
      "3d72a2698104: Verifying Checksum\n",
      "3d72a2698104: Download complete\n",
      "7d3cd81654d5: Verifying Checksum\n",
      "7d3cd81654d5: Download complete\n",
      "aeff973c2191: Verifying Checksum\n",
      "aeff973c2191: Download complete\n",
      "85d7d3ff0cca: Verifying Checksum\n",
      "85d7d3ff0cca: Download complete\n",
      "5996430251dd: Verifying Checksum\n",
      "5996430251dd: Download complete\n",
      "314dc83fdfc2: Download complete\n",
      "927db4ce3e96: Verifying Checksum\n",
      "927db4ce3e96: Download complete\n",
      "cbe4a04f4491: Download complete\n",
      "ddb02ef514f8: Verifying Checksum\n",
      "ddb02ef514f8: Download complete\n",
      "a76bf8c63bc7: Verifying Checksum\n",
      "a76bf8c63bc7: Download complete\n",
      "4aadc1bd8c8a: Verifying Checksum\n",
      "4aadc1bd8c8a: Download complete\n",
      "268bcca40b76: Verifying Checksum\n",
      "268bcca40b76: Download complete\n",
      "83c9f4bab802: Verifying Checksum\n",
      "83c9f4bab802: Download complete\n",
      "5cef8f59ae9a: Verifying Checksum\n",
      "5cef8f59ae9a: Download complete\n",
      "0a4990a32bb5: Verifying Checksum\n",
      "0a4990a32bb5: Download complete\n",
      "af7b4f7dc15a: Verifying Checksum\n",
      "af7b4f7dc15a: Download complete\n",
      "054e2ffff644: Pull complete\n",
      "7d3cd81654d5: Pull complete\n",
      "dca613dca886: Pull complete\n",
      "0fdcdcda3b2e: Pull complete\n",
      "af7b4f7dc15a: Pull complete\n",
      "6d101782f66c: Pull complete\n",
      "e8427cb13897: Pull complete\n",
      "de05b029a5a2: Pull complete\n",
      "3d72a2698104: Pull complete\n",
      "aeff973c2191: Pull complete\n",
      "85d7d3ff0cca: Pull complete\n",
      "5996430251dd: Pull complete\n",
      "314dc83fdfc2: Pull complete\n",
      "5cef8f59ae9a: Pull complete\n",
      "927db4ce3e96: Pull complete\n",
      "cbe4a04f4491: Pull complete\n",
      "ddb02ef514f8: Pull complete\n",
      "a76bf8c63bc7: Pull complete\n",
      "4aadc1bd8c8a: Pull complete\n",
      "268bcca40b76: Pull complete\n",
      "83c9f4bab802: Pull complete\n",
      "0a4990a32bb5: Pull complete\n",
      "Digest: sha256:b3e6ffe10fc22ec809e786efb36335ff40b21a4fd6b1b893556b5085822db4c0\n",
      "Status: Downloaded newer image for public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\n",
      "public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\n",
      "Resolved account: 570598552974\n",
      "Resolved region: us-west-2\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "The push refers to repository [570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-llama3-70b-instruct]\n",
      "e60486e118b3: Preparing\n",
      "ac8261e8f418: Preparing\n",
      "b3cac77c4405: Preparing\n",
      "3dcb4fe8eff3: Preparing\n",
      "63aa9863055a: Preparing\n",
      "ec52320e4279: Preparing\n",
      "ec561d7e6811: Preparing\n",
      "64698ac07231: Preparing\n",
      "086db532493b: Preparing\n",
      "96ffa74d03cf: Preparing\n",
      "029474febabe: Preparing\n",
      "591935392904: Preparing\n",
      "22ba5c2fc887: Preparing\n",
      "9034ce09b708: Preparing\n",
      "034e11c3e122: Preparing\n",
      "80e3915a53fa: Preparing\n",
      "2c9f8812e444: Preparing\n",
      "e9878d508160: Preparing\n",
      "e6a2e7dc7c95: Preparing\n",
      "57c2269cdf89: Preparing\n",
      "aa02d223d9df: Preparing\n",
      "17bbf87bba59: Preparing\n",
      "64698ac07231: Waiting\n",
      "3970fd90d179: Preparing\n",
      "086db532493b: Waiting\n",
      "bae3163c64b8: Preparing\n",
      "f0fc8a1ca0cb: Preparing\n",
      "96ffa74d03cf: Waiting\n",
      "029474febabe: Waiting\n",
      "90efea7ecd8e: Preparing\n",
      "b6a0147bcf99: Preparing\n",
      "591935392904: Waiting\n",
      "8ceb9643fb36: Preparing\n",
      "22ba5c2fc887: Waiting\n",
      "9034ce09b708: Waiting\n",
      "e6a2e7dc7c95: Waiting\n",
      "57c2269cdf89: Waiting\n",
      "aa02d223d9df: Waiting\n",
      "17bbf87bba59: Waiting\n",
      "034e11c3e122: Waiting\n",
      "3970fd90d179: Waiting\n",
      "80e3915a53fa: Waiting\n",
      "bae3163c64b8: Waiting\n",
      "ec52320e4279: Waiting\n",
      "2c9f8812e444: Waiting\n",
      "f0fc8a1ca0cb: Waiting\n",
      "90efea7ecd8e: Waiting\n",
      "e9878d508160: Waiting\n",
      "b6a0147bcf99: Waiting\n",
      "8ceb9643fb36: Waiting\n",
      "ec561d7e6811: Waiting\n",
      "b3cac77c4405: Pushed\n",
      "63aa9863055a: Pushed\n",
      "ac8261e8f418: Pushed\n",
      "3dcb4fe8eff3: Pushed\n",
      "ec52320e4279: Pushed\n",
      "64698ac07231: Pushed\n",
      "ec561d7e6811: Pushed\n",
      "029474febabe: Pushed\n",
      "96ffa74d03cf: Pushed\n",
      "591935392904: Pushed\n",
      "22ba5c2fc887: Pushed\n",
      "9034ce09b708: Pushed\n",
      "034e11c3e122: Pushed\n",
      "80e3915a53fa: Pushed\n",
      "2c9f8812e444: Pushed\n",
      "086db532493b: Pushed\n",
      "e6a2e7dc7c95: Pushed\n",
      "e60486e118b3: Pushed\n",
      "3970fd90d179: Pushed\n",
      "bae3163c64b8: Pushed\n",
      "f0fc8a1ca0cb: Pushed\n",
      "90efea7ecd8e: Pushed\n",
      "b6a0147bcf99: Pushed\n",
      "8ceb9643fb36: Pushed\n",
      "e9878d508160: Pushed\n",
      "57c2269cdf89: Pushed\n",
      "aa02d223d9df: Pushed\n",
      "17bbf87bba59: Pushed\n",
      "latest: digest: sha256:b3e6ffe10fc22ec809e786efb36335ff40b21a4fd6b1b893556b5085822db4c0 size: 6184\n",
      "570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-llama3-70b-instruct\n",
      "Errors: WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error getting AWS account ID: {result.stderr}\")\n",
    "else:\n",
    "    account = result.stdout.strip()\n",
    "    print(f\"AWS account ID: {account}\")\n",
    "\n",
    "bash_script = f\"\"\"\n",
    "echo \"Public NIM Image: {public_nim_image}\"\n",
    "docker pull {public_nim_image}\n",
    "\n",
    "\n",
    "echo \"Resolved account: {account}\"\n",
    "echo \"Resolved region: {region}\"\n",
    "\n",
    "nim_image=\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "\n",
    "# Ensure the repository name adheres to AWS constraints\n",
    "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "docker tag {public_nim_image} $nim_image\n",
    "docker push $nim_image\n",
    "echo -n $nim_image\n",
    "\"\"\"\n",
    "nim_image=f\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "# Run the bash script and capture real-time output\n",
    "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == b'' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.decode().strip())\n",
    "\n",
    "stderr = process.stderr.read().decode()\n",
    "if stderr:\n",
    "    print(\"Errors:\", stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863fd4-0893-4022-9ab0-38e1af1512d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We print the private ECR NIM image in your account that we will be using for SageMaker deployment. \n",
    "- Should be similar to  `\"<ACCOUNT ID>.dkr.ecr.<REGION>.amazonaws.com/<NIM_MODEL>:latest\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bb63ac-2a7a-4beb-b0dd-77a4473e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570598552974.dkr.ecr.us-west-2.amazonaws.com/nim-llama3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "print(nim_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8451e1-5683-4e88-995c-3a0250c99e39",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9efc86-0cf2-403b-9502-fd294acb4cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before proceeding further, please set your NGC API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f1f264-ebd8-4c6a-9926-7a21afd89ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set your NGC API key here\n",
    "NGC_API_KEY = \"SET YOUR NGC API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb9b93a-3fdf-49a4-8f89-494238008a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable. It's required for running NIM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5bce6-3807-43c3-866f-80543cfdedbf",
   "metadata": {},
   "source": [
    "We define the sagemaker model from the NIM container making sure to pass in **NGC_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-west-2:570598552974:model/nim-llama3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image,\n",
    "    \"Environment\": {\"NGC_API_KEY\": NGC_API_KEY}\n",
    "}\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2f0e6-0377-4a13-9cc3-c345adf08c86",
   "metadata": {},
   "source": [
    "Next we create endpoint configuration, here we are deploying the Llama model on the specified instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-west-2:570598552974:endpoint-config/nim-llama3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 850\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-west-2:570598552974:endpoint/nim-llama3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = sm_model_name\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:570598552974:endpoint/nim-llama3-70b-instruct\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "## Test Inference and Streaming Inference with Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36583-d6b0-4fdf-a659-c088f913034a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>NOTE:</b> The model's name in the inference request payload needs to be the name of the NIM model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"cmpl-304a3a7588be4edf83e96a3f3ea7720f\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1722013379,\n",
      "  \"model\": \"meta/llama3-70b-instruct\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Optimum Neuron is an open-source library developed by Hugging Face that enables efficient compilation and deployment of Large Language Models (LLMs) on various infrastructure platforms, including AWS. Here's a detailed breakdown of how Optimum Neuron helps compile LLMs for AWS infrastructure:\\n\\n**Overview of the compilation process**\\n\\nWhen deploying an LLM on AWS infrastructure, the model needs to be optimized and compiled to run efficiently on the target hardware. This involves several steps:\\n\\n1. **Model loading**: The LLM is loaded from a model hub or a local file.\\n2. **Model optimization**: The model is optimized for inference using techniques like pruning, quantization, and knowledge distillation to reduce its computational requirements.\\n3. **Model compilation**: The optimized model is compiled into an executable format that can run on the target AWS infrastructure.\\n\\n**Optimum Neuron's role in compilation**\\n\\nOptimum Neuron provides a uniform interface for compiling and deploying LLMs on various infrastructure platforms, including AWS. It takes care of the compilation process, allowing users to focus on developing and fine-tuning their models. Here's how Optimum Neuron helps compile LLMs for AWS infrastructure:\\n\\n**1. Model loading and optimization**\\n\\nOptimum Neuron loads the LLM from a model hub or a local file using the Hugging Face Transformers library. It then applies various optimization techniques to reduce the model's computational requirements, making it more suitable for deployment on AWS infrastructure. These techniques include:\\n\\n* **Pruning**: Removing redundant or unnecessary weights to reduce the model's size and computational requirements.\\n* **Quantization**: Representing model weights and activations using fewer bits (e.g., int8 instead of float32) to reduce memory usage and improve performance.\\n* **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller, more efficient model.\\n\\n**2. Compilation for AWS infrastructure**\\n\\nOptimum Neuron compiles the optimized model into an executable format that can run on AWS infrastructure. This involves generating optimized machine code for the target AWS hardware, such as Amazon EC2 instances or AWS Inferentia chips. The compilation process is done using the following components:\\n\\n* **Neuron Compiler**: A component of Optimum Neuron that compiles the optimized model into an intermediate representation (IR) using the Neuron compiler.\\n* **AWS Neuron SDK**: A software development kit provided by AWS that allows developers to run Neuron-compiled models on AWS infrastructure. The AWS Neuron SDK provides a set of libraries and tools for compiling, optimizing, and running neural networks on AWS hardware.\\n\\n**3. Deployment on AWS infrastructure**\\n\\nOnce the model is compiled, Optimum Neuron deploys the executable model on the target AWS infrastructure, such as Amazon EC2 instances or AWS Inferentia chips. This involves:\\n\\n* **Containerization**: The compiled model is packaged into a container using Docker or other containerization tools, ensuring that the model and its dependencies are encapsulated and easily deployable.\\n* **Orchestration**: The containerized model is deployed on the target AWS infrastructure using orchestration tools like Kubernetes or AWS ECS.\\n\\n**Benefits of using Optimum Neuron for compiling LLMs on AWS**\\n\\nOptimum Neuron provides several benefits for compiling and deploying LLMs on AWS infrastructure, including:\\n\\n* **Efficient compilation**: Optimum Neuron's compilation process is optimized for performance, reducing the time and resources required for deployment.\\n* **Unified interface**: Optimum Neuron provides a uniform interface for compiling and deploying LLMs on various infrastructure platforms, making it easier to switch between platforms or deploy models on multiple platforms.\\n* **Optimized for AWS infrastructure**: Optimum Neuron is designed to work seamlessly with AWS infrastructure, ensuring that models are optimized for the target hardware and take advantage of AWS-specific features and optimizations.\\n\\nBy using Optimum Neuron, developers can focus on developing and fine-tuning their LLMs, while the library takes care of the compilation and deployment process, ensuring efficient and scalable deployment on AWS infrastructure.\"\n",
      "      },\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"stop_reason\": 128009\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 59,\n",
      "    \"total_tokens\": 875,\n",
      "    \"completion_tokens\": 816\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail how Optimum Neuron helps compile LLMs for AWS infrastructure\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1112217-3d99-409c-8329-b8e86c17782b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Neuron is an open-source library developed by Hugging Face that enables efficient compilation and deployment of Large Language Models (LLMs) on various infrastructure platforms, including AWS. Here's a detailed breakdown of how Optimum Neuron helps compile LLMs for AWS infrastructure:\n",
      "\n",
      "**Overview of the compilation process**\n",
      "\n",
      "When deploying an LLM on AWS infrastructure, the model needs to be optimized and compiled to run efficiently on the target hardware. This involves several steps:\n",
      "\n",
      "1. **Model loading**: The LLM is loaded from a model hub or a local file.\n",
      "2. **Model optimization**: The model is optimized for inference using techniques like pruning, quantization, and knowledge distillation to reduce its computational requirements.\n",
      "3. **Model compilation**: The optimized model is compiled into an executable format that can run on the target AWS infrastructure.\n",
      "\n",
      "**Optimum Neuron's role in compilation**\n",
      "\n",
      "Optimum Neuron provides a uniform interface for compiling and deploying LLMs on various infrastructure platforms, including AWS. It takes care of the compilation process, allowing users to focus on developing and fine-tuning their models. Here's how Optimum Neuron helps compile LLMs for AWS infrastructure:\n",
      "\n",
      "**1. Model loading and optimization**\n",
      "\n",
      "Optimum Neuron loads the LLM from a model hub or a local file using the Hugging Face Transformers library. It then applies various optimization techniques to reduce the model's computational requirements, making it more suitable for deployment on AWS infrastructure. These techniques include:\n",
      "\n",
      "* **Pruning**: Removing redundant or unnecessary weights to reduce the model's size and computational requirements.\n",
      "* **Quantization**: Representing model weights and activations using fewer bits (e.g., int8 instead of float32) to reduce memory usage and improve performance.\n",
      "* **Knowledge distillation**: Transferring knowledge from a large, pre-trained model to a smaller, more efficient model.\n",
      "\n",
      "**2. Compilation for AWS infrastructure**\n",
      "\n",
      "Optimum Neuron compiles the optimized model into an executable format that can run on AWS infrastructure. This involves generating optimized machine code for the target AWS hardware, such as Amazon EC2 instances or AWS Inferentia chips. The compilation process is done using the following components:\n",
      "\n",
      "* **Neuron Compiler**: A component of Optimum Neuron that compiles the optimized model into an intermediate representation (IR) using the Neuron compiler.\n",
      "* **AWS Neuron SDK**: A software development kit provided by AWS that allows developers to run Neuron-compiled models on AWS infrastructure. The AWS Neuron SDK provides a set of libraries and tools for compiling, optimizing, and running neural networks on AWS hardware.\n",
      "\n",
      "**3. Deployment on AWS infrastructure**\n",
      "\n",
      "Once the model is compiled, Optimum Neuron deploys the executable model on the target AWS infrastructure, such as Amazon EC2 instances or AWS Inferentia chips. This involves:\n",
      "\n",
      "* **Containerization**: The compiled model is packaged into a container using Docker or other containerization tools, ensuring that the model and its dependencies are encapsulated and easily deployable.\n",
      "* **Orchestration**: The containerized model is deployed on the target AWS infrastructure using orchestration tools like Kubernetes or AWS ECS.\n",
      "\n",
      "**Benefits of using Optimum Neuron for compiling LLMs on AWS**\n",
      "\n",
      "Optimum Neuron provides several benefits for compiling and deploying LLMs on AWS infrastructure, including:\n",
      "\n",
      "* **Efficient compilation**: Optimum Neuron's compilation process is optimized for performance, reducing the time and resources required for deployment.\n",
      "* **Unified interface**: Optimum Neuron provides a uniform interface for compiling and deploying LLMs on various infrastructure platforms, making it easier to switch between platforms or deploy models on multiple platforms.\n",
      "* **Optimized for AWS infrastructure**: Optimum Neuron is designed to work seamlessly with AWS infrastructure, ensuring that models are optimized for the target hardware and take advantage of AWS-specific features and optimizations.\n",
      "\n",
      "By using Optimum Neuron, developers can focus on developing and fine-tuning their LLMs, while the library takes care of the compilation and deployment process, ensuring efficient and scalable deployment on AWS infrastructure.\n"
     ]
    }
   ],
   "source": [
    "content = output[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me in detail what inference engines and llm serving frameworks are\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d24a-a23e-40aa-afdb-da2a63f624fe",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e05742b6-f8e4-4116-92ed-e9b156b6cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to explain inference engines and LLM serving frameworks in detail.\n",
      "\n",
      "**Inference Engines**\n",
      "\n",
      "An inference engine is a software component that enables machine learning models to make predictions or take actions based on input data. It is responsible for executing the model's computations and generating output. In other words, an inference engine is the \"brain\" that powers a machine learning model's decision-making process.\n",
      "\n",
      "Inference engines are typically designed to work with specific types of machine learning models, such as neural networks, decision trees, or linear models. They can be used for various tasks, including:\n",
      "\n",
      "1. ** Prediction**: Given input data, an inference engine can generate predictions or outputs based on the trained model.\n",
      "2. **Scoring**: An inference engine can calculate the output score or probability of a particular input.\n",
      "3. **Classification**: It can classify input data into predefined categories or labels.\n",
      "\n",
      "An inference engine usually performs the following tasks:\n",
      "\n",
      "1. **Model loading**: It loads the trained machine learning model into memory.\n",
      "2. **Input preprocessing**: It prepares the input data for the model by normalizing, transforming, or feature engineering.\n",
      "3. **Model execution**: It executes the model's computations to generate output.\n",
      "4. **Output postprocessing**: It may perform additional processing on the output, such as thresholding or formatting.\n",
      "\n",
      "There are several types of inference engines, including:\n",
      "\n",
      "1. **TensorFlow Inference Engine**: Designed for TensorFlow models, it provides optimized performance for deep learning models.\n",
      "2. **OpenVINO**: A open-source inference engine that supports various frameworks, including TensorFlow, Caffe, and PyTorch.\n",
      "3. **TensorRT**: A high-performance inference engine developed by NVIDIA, optimized for GPU acceleration.\n",
      "\n",
      "**LLM Serving Frameworks**\n",
      "\n",
      "A Large Language Model (LLM) serving framework is a software infrastructure designed to efficiently deploy, manage, and serve large language models in production environments. These frameworks enable developers to build scalable, performant, and reliable applications that utilize the capabilities of LLMs.\n",
      "\n",
      "LLM serving frameworks typically provide the following features:\n",
      "\n",
      "1. **Model management**: They allow for easy model deployment, versioning, and updating.\n",
      "2. **Scalability**: They enable horizontal scaling to handle large volumes of requests and traffic.\n",
      "3. **Performance optimization**: They provide optimized performance for LLMs, often using GPU acceleration and caching.\n",
      "4. **Request handling**: They manage incoming requests, perform input preprocessing, and generate output.\n",
      "5. **Security and authentication**: They ensure secure access to the LLM and provide authentication mechanisms.\n",
      "\n",
      "Some popular LLM serving frameworks include:\n",
      "\n",
      "1. **Hugging Face Transformers**: A widely-used open-source framework for serving transformer-based LLMs, such as BERT and RoBERTa.\n",
      "2. **TensorFlow Serving**: A serving platform that supports a wide range of machine learning models, including LLMs.\n",
      "3. **AWS SageMaker**: A cloud-based platform that provides a managed service for deploying and serving LLMs.\n",
      "4. **Microsoft Azure Machine Learning**: A cloud-based platform that offers a managed service for deploying and serving LLMs.\n",
      "\n",
      "In summary, inference engines are software components that execute machine learning models to generate predictions or outputs, while LLM serving frameworks are software infrastructures designed to deploy, manage, and serve large language models in production environments."
     ]
    }
   ],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "---\n",
    "### Delete endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '90b9f892-7fe9-446d-913e-31f52049dbd1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '90b9f892-7fe9-446d-913e-31f52049dbd1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Fri, 26 Jul 2024 17:03:52 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f467d8-a9ff-49d7-9425-37e83e54cd19",
   "metadata": {},
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
