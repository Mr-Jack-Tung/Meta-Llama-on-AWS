{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Long Document Summarization with Llama3 on Bedrock with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate using the [Llama3 family of models](https://aws.amazon.com/about-aws/whats-new/2024/04/meta-llama-3-foundation-models-aws/) on Amazon Bedrock for abstract document summarization tasks. Although the Llama3 8B and 70B models are powerful and versatile language models that can be used for a wide range of natural language processing tasks, they have relatively small context window sizes compared to other models in the class. As a result, when working with multiple large documents there are several challenges that can arise. One of the main challenges is that the input text might exceed the model's context length of 8k tokens. This limitation can lead to incomplete or inaccurate responses, as the model may not have access to all the relevant information within the document. Another challenge is that language models can sometimes hallucinate or generate factually incorrect responses when dealing with very long documents. This can happen because the model may lose track of the overall context or make incorrect inferences based on partial information. Additionally, processing large documents can lead to out-of-memory errors, especially on resource-constrained systems or when working with large language models that have high memory requirements.\n",
    "\n",
    "To address these challenges, this notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "## Llama 3 Model Selection\n",
    "\n",
    "Today, there are two Llama 3 models available on Amazon Bedrock:\n",
    "\n",
    "### 1. Llama 3 8B\n",
    "\n",
    "- **Description:** Ideal for limited computational power and resources, faster training times, and edge devices.\n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Text Classification, and Sentiment Analysis.\n",
    "\n",
    "### 2. Llama 3 70B\n",
    "\n",
    "- **Description:** Ideal for content creation, conversational AI, language understanding, research development, and enterprise applications. \n",
    "- **Max Tokens:** 2,048\n",
    "- **Context Window:** 8,196\n",
    "- **Languages:** English\n",
    "- **Supported Use Cases:** Synthetic Text Generation and Accuracy, Text Classification and Nuance, Sentiment Analysis and Nuance Reasoning, Language Modeling, Dialogue Systems, and Code Generation.\n",
    "\n",
    "### Performance and Cost Trade-offs\n",
    "\n",
    "The table below compares the model performance on the Massive Multitask Language Understanding (MMLU) benchmark and their on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Llama 3 8B | 68.4%      | \\$0.0004                   | \\$0.0006                    |\n",
    "| Llama 3 70B | 82.0%      | \\$0.00265                   | \\$0.0035                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Llama 3 8B Model Cards and Prompt Formats](https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3==1.34.58\n",
    "botocore==1.34.101\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {},
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', \n",
    "                       region_name='us-east-1',\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Ensure that you have access to the Llama3 model you wish to use through Bedrock.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Mistral Large in `model_id` and pass Llama3 inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "### Supported parameters\n",
    "\n",
    "The Llama models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"max_gen_len\": int\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired Llama3 model\n",
    "llama3_70b_instruct = \"meta.llama3-70b-instruct-v1:0\"\n",
    "llama3_8b_instruct = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = llama3_70b_instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_gen_len\": 2048,  \n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's lovely to chat with you. I'm an AI, and I've been trained on a vast amount of text data, which I'm excited to draw upon to converse with you. I've been online for about 37 minutes and 14 seconds now, and I've already processed over 12,000 words of input. My systems are functioning within optimal parameters, and I'm ready to engage in a fun and informative conversation with you. What would you like to talk about?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize conversation chain \n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the execution of the conversation chain. This can be set to true when you're debugging your conversation chain or trying to understand how it's working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory() \n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8307f-d16d-427a-b9e7-d7deac5cdb05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {},
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "In this example, to demonstrate summarization, we will be using two documents that are both whitepapers from AWS. \n",
    "\n",
    "> The first document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS.\n",
    "\n",
    "> The second document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf) about containers on AWS. \n",
    "\n",
    "Let's first download these files to build our document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf',\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf',\n",
    "    'AWS-containers-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AWS Whitepaper\\nArchitecting for HIPAA Security and \\nCompliance on Amazon Web Services\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.' metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the HIPAA Security document: 152\n"
     ]
    }
   ],
   "source": [
    "#document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "#chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d759ea-f15f-4acd-a60d-15abc519135f",
   "metadata": {},
   "source": [
    "#### Containers on AWS Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb32bc-e7b4-488d-bdff-a52b6dc42e45",
   "metadata": {},
   "source": [
    "In this section, we will load the Containers on AWS document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `container_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12233e31-3079-4128-8ff2-0e79d07e2461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Containers on AWS AWS Whitepaper\\nContainers on AWS: AWS Whitepaper\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\\nAmazon's trademarks and trade dress may not be used in connection with any product or service \\nthat is not Amazon's, in any manner that is likely to cause confusion among customers, or in any \\nmanner that disparages or discredits Amazon. All other trademarks not owned by Amazon are \\nthe property of their respective owners, who may or may not be aﬃliated with, connected to, or \\nsponsored by Amazon.\" metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the original: 57\n"
     ]
    }
   ],
   "source": [
    "#document 2 (Containers on AWS)\n",
    "container_documents = []\n",
    "\n",
    "# Load only the second file\n",
    "container_file = filenames[1]\n",
    "container_loader = PyPDFLoader(data_root + container_file)\n",
    "container_document = container_loader.load()\n",
    "\n",
    "for idx, container_document_fragment in enumerate(container_document):\n",
    "    container_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    container_documents.append(container_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "container_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "container_docs = container_text_splitter.split_documents(container_documents)\n",
    "print(container_docs[1])\n",
    "\n",
    "#chunked doc count\n",
    "container_chunked_count = len(container_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the original: {container_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a712e-99d8-403d-85a2-cfa048ceaabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {},
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {},
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want. It is the default way to process documents with an LLM.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the **Containers on AWS** whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: This model's maximum context length is 8192 tokens. Please reduce the length of the prompt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_container_summary = stuff_summary_chain.invoke(container_docs) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we see that an error is raised due to the prompt far exceeding the model's maximum context length. Since stuffing summarizes text by feeding the entire document to a large language model (LLM) in a single call, it is difficult to process long documents. The Llama models have a context length of 8k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html) to be able to mitigate the restrictions of your large documents not fitting into the context window of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. Model generates individual summaries for all document chunks in parallel\n",
    "4. Reduce all these summaries to a condensed final summary\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/llama3mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain, it then combines and iteratively reduces the mapped document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "#sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "#sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt= PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:71])  \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a detailed summary of the text in bullet points, covering the key points of the text:\n",
      "\n",
      "**EFS (Elastic File System) Security and Compliance**\n",
      "\n",
      "* PHI must be encrypted at-rest on EFS using AES-256 encryption and AWS KMS-managed keys.\n",
      "* Customers are responsible for managing encryption and key management if they encrypt data before placing it on EFS.\n",
      "* PHI should not be used in file names or folder names.\n",
      "* Encryption of PHI in transit is provided by Transport Layer Security (TLS) between the EFS service and the instance mounting the file system.\n",
      "\n",
      "**Amazon Elastic Kubernetes Service (Amazon EKS)**\n",
      "\n",
      "* Amazon EKS is a managed service that makes it easy to run Kubernetes on AWS without maintaining a Kubernetes control plane.\n",
      "* Refer to the \"Architecting for HIPAA Security and Compliance on Amazon EKS\" whitepaper for additional Security and Compliance information.\n",
      "\n",
      "**Amazon ElastiCache for Redis**\n",
      "\n",
      "* To store PHI, customers must ensure they are running the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types.\n",
      "* Supported node types include M4, M5, R4, R5, T2, and T3.\n",
      "* Supported ElastiCache for Redis engine versions include 3.2.6 and 4.0.10 onwards.\n",
      "* Customers must configure ElastiCache for Redis clusters and nodes to:\n",
      "\t+ Encrypt data at rest\n",
      "\t+ Enable transport encryption\n",
      "\t+ Enable authentication of Redis commands\n",
      "\t+ Apply the latest 'Security' type service updates on or before the 'Recommended Apply by Date'\n",
      "\n",
      "**HIPAA Security and Compliance on Amazon Web Services**\n",
      "\n",
      "* To ensure HIPAA compliance, customers must configure Amazon ElastiCache for Redis clusters and nodes to meet specific requirements.\n",
      "\n",
      "**Encryption at Rest**\n",
      "\n",
      "* Amazon ElastiCache for Redis provides data encryption for clusters to protect data at rest.\n",
      "* Encryption at rest is enabled at cluster creation time.\n",
      "* Data on disk is encrypted using hardware-accelerated Advanced Encryption Standard (AES)-512 symmetric keys.\n",
      "* Redis backups are encrypted through Amazon S3-managed encryption keys (SSE-S3) using hardware-accelerated AES-256 symmetric keys.\n",
      "\n",
      "**Transport Encryption**\n",
      "\n",
      "* Amazon ElastiCache for Redis uses Transport Layer Security (TLS) to encrypt data in transit.\n",
      "* Connections to ElastiCache for Redis containing PHI must use transport encryption.\n",
      "\n",
      "**Authentication**\n",
      "\n",
      "* Amazon ElastiCache for Redis clusters containing PHI must provide a Redis AUTH token for authentication.\n",
      "* Redis AUTH is available when both encryption at-rest and encryption-in-transit are enabled.\n",
      "* The AUTH token must meet specific requirements.\n",
      "\n",
      "**Applying ElastiCache Service Updates**\n",
      "\n",
      "* Amazon ElastiCache for Redis clusters containing PHI must be updated with the latest 'Security' type service updates.\n",
      "* Updates must be applied on or before the 'Recommended Apply by Date'.\n",
      "\n",
      "**Amazon OpenSearch Service**\n",
      "\n",
      "* Enables customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC).\n",
      "* When using OpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later.\n",
      "* PHI should be encrypted at-rest and in-transit within Amazon OpenSearch Service.\n",
      "* Customers can use AWS KMS key encryption to encrypt data at rest in their OpenSearch Service domains.\n",
      "\n",
      "**Other AWS Services**\n",
      "\n",
      "* Amazon EMR: deploys and manages a cluster of Amazon EC2 instances into a customer's account; see Encryption Options for information on encryption with Amazon EMR.\n",
      "* Amazon EventBridge: a serverless event bus that enables the creation of scalable event-driven applications.\n",
      "* Amazon Forecast: a fully managed service using machine learning for accurate forecasts; every interaction with Amazon Forecast is protected by encryption.\n",
      "* Amazon FSx: provides high-performance storage for compute workloads and is powered by Lustre, the world's most popular high-performance file system.\n",
      "* Amazon GuardDuty: a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help customers protect their AWS accounts and workloads.\n",
      "\n",
      "**Amazon GuardDuty**\n",
      "\n",
      "* Does not encounter Protected Health Information (PHI) as it is not stored in AWS-based data sources.\n",
      "\n",
      "**Amazon HealthLake**\n",
      "\n",
      "* Enables customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale.\n",
      "* Allows customers to transmit, process, and store PHI.\n",
      "* Encrypts data at rest in customer's data stores by default.\n",
      "* All service data and metadata is encrypted with a service-owned KMS key.\n",
      "* Follows Fast Healthcare Interoperability Resources (FHIR) specifications.\n",
      "\n",
      "**Data Deletion and Retention**\n",
      "\n",
      "* When a FHIR resource is deleted, it is only hidden from retrieval and retained by the service for versioning purposes.\n",
      "* When using the StartFHIRImportJob API, Amazon HealthLake enforces the requirement to export data to an encrypted Amazon S3 bucket.\n",
      "\n",
      "**Data Encryption**\n",
      "\n",
      "* Encrypts data both in transit and at rest.\n",
      "* For data in transit, clients must support Transport Layer Security (TLS) 1.0 or later, with a recommendation of TLS 1.3.\n",
      "* Clients must also support cipher suites with perfect forward secrecy (PFS) such as Ephemeral Diffie-Hellman (DHE) or Elliptic Curve Ephemeral Diffie-Hellman (ECDHE).\n",
      "* Requests must be signed using an access key ID and a secret access key associated with an IAM principal, or using temporary security credentials generated by AWS STS.\n",
      "\n",
      "**Data at Rest Encryption**\n",
      "\n",
      "* Encrypts data in customer's data stores with a customer-owned AWS KMS key or a service-owned AWS KMS key by default.\n",
      "* All service data and metadata is encrypted at rest with a service-owned AWS KMS key.\n",
      "\n",
      "**Integration with AWS CloudTrail**\n",
      "\n",
      "* Captures all API calls to Amazon HealthLake as events.\n",
      "* Events include calls made through the AWS Management Console, command line interface (CLI), and programmatically using software development kits (SDKs).\n",
      "\n",
      "**Amazon Inspector**\n",
      "\n",
      "* Assesses applications for vulnerabilities and deviations from best practices.\n",
      "* Produces a detailed list of security findings prioritized by level of severity after performing an assessment.\n",
      "* Can be run on EC2 instances that contain Protected Health Information (PHI).\n",
      "* Encrypts all data transmitted over the network and all telemetry data stored at-rest.\n",
      "\n",
      "**Amazon Managed Service for Apache Flink**\n",
      "\n",
      "* Enables customers to quickly author SQL code that continuously reads, processes, and stores data in near real-time.\n",
      "* Allows customers to construct applications that transform and provide insights into their data using standard SQL queries on streaming data.\n",
      "* Supports inputs from Kinesis Data Streams.\n",
      "* Can access encrypted data in Firehose delivery streams seamlessly without further configuration.\n",
      "* Does not store unencrypted data read from Kinesis Data Streams.\n",
      "* Integrates with AWS CloudTrail and Amazon CloudWatch Logs for application monitoring.\n",
      "\n",
      "**Amazon Data Firehose**\n",
      "\n",
      "* Encrypts data using an AWS KMS key before storing it at-rest in Kinesis Data Streams.\n",
      "* Decrypts data and sends it to Firehose, which buffers it in memory based on customer-specified buffering hints.\n",
      "* Delivers data to destinations without storing unencrypted data at rest.\n",
      "* Provides various monitoring tools, including Amazon CloudWatch metrics, Amazon CloudWatch Logs, Kinesis Agent, and API logging and history.\n",
      "\n",
      "**Amazon Kinesis Streams**\n",
      "\n",
      "* Enables customers to build custom applications that process or analyze streaming data for specialized needs.\n",
      "* Supports server-side encryption, which encrypts data at rest using an AWS KMS key before storing it on disks.\n",
      "\n",
      "**Data Protection in Amazon Kinesis Data Streams**\n",
      "\n",
      "* Use a KMS key to encrypt data before storing it on disks.\n",
      "* Connections to Amazon S3 containing Protected Health Information (PHI) must use endpoints that accept encrypted transport (HTTPS).\n",
      "* Regional endpoints can be found in AWS service endpoints.\n",
      "\n",
      "**Amazon Kinesis Video Streams**\n",
      "\n",
      "* A fully managed AWS service for streaming live video from devices to the AWS Cloud.\n",
      "* Enables building applications for real-time video processing or analysis.\n",
      "* Automatically encrypts data at rest using an AWS KMS key specified by the customer.\n",
      "* Encrypts data before writing to storage and decrypts after retrieval.\n",
      "* SDK uses TLS to encrypt frames and fragments generated by hardware devices.\n",
      "* Does not manage or affect data stored at-rest.\n",
      "* Uses AWS CloudTrail to log all API calls.\n",
      "\n",
      "**Amazon Lex**\n",
      "\n",
      "* Enables customers to build conversational interfaces for applications using voice and text.\n",
      "* Uses HTTPS protocol to communicate with clients and other AWS services.\n",
      "* Access is API-driven, and IAM least privilege can be enforced.\n",
      "* Integrates with CloudWatch to provide metrics for individual operations or global operations for an account.\n",
      "* Customers can set up CloudWatch alarms to be notified when metrics exceed a defined threshold.\n",
      "\n",
      "**Amazon Managed Streaming for Apache Kafka (Amazon MSK)**\n",
      "\n",
      "* Provides encryption features for data at rest (using Amazon EBS server-side encryption and AWS KMS keys) and data in-transit.\n",
      "* Clusters have encryption enabled via TLS for inter-broker communication.\n",
      "* Encryption configuration setting is enabled by default when a cluster is created.\n",
      "* In-transit encryption is set to TLS for clusters created from CLI or AWS Console.\n",
      "* Customers can change the default encryption setting by selecting TLS/plaintext settings.\n",
      "* Supports monitoring and performance using Amazon MSK console, Amazon CloudWatch console, and Open Monitoring with Prometheus.\n",
      "\n",
      "**Apache Zookeeper and Apache Kafka**\n",
      "\n",
      "* The default version of Apache Zookeeper bundled with Apache Kafka does not support encryption.\n",
      "* Communications between Apache Zookeeper and Apache Kafka brokers are limited to broker, topic, and partition state information.\n",
      "* Data can only be produced and consumed from an Amazon MSK cluster over a private connection between clients in their VPC and the Amazon MSK cluster.\n",
      "\n",
      "**Amazon MQ**\n",
      "\n",
      "* A managed message broker service for Apache ActiveMQ.\n",
      "* Encrypts messages at-rest and in-transit using managed encryption keys.\n",
      "* Logs all API calls using CloudTrail.\n",
      "* Supports STOMP over WebSocket.\n",
      "\n",
      "**Amazon Neptune**\n",
      "\n",
      "* A fast, reliable,\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {},
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method is a technique that allows us to recursively summarize our input data. It iteratively updates its answer by looping over the input documents. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce`. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. First chunk is sent to the model; Model returns the corresponding summary\n",
    "4. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have final summary that has been recursively updated using all the document chunks\n",
    "\n",
    "---\n",
    "\n",
    "![refine](imgs/llamarefine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run an initial prompt on a small chunk of data to generate a summary. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False) #refine summary chain for summarization in french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "#refine llm chain prompt template\n",
    "refine_documents_prompt= PromptTemplate.from_template(\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    refine_summary = refine_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the new context, I did not find any new information that is relevant to the original summary. The new context appears to discuss Amazon MQ, Amazon Neptune, and AWS Network Firewall, which are not mentioned in the original summary. Therefore, I will return the original summary as it is.\n",
      "\n",
      "Here is the original summary:\n",
      "\n",
      "**Main Points:**\n",
      "\n",
      "* Amazon Web Services (AWS) provides guidance on architecting for HIPAA security and compliance on its platform.\n",
      "* The paper discusses ten AWS services: Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EMR, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, and Amazon HealthLake, as well as two additional services: Amazon Inspector and Amazon Managed Service for Apache Flink.\n",
      "\n",
      "**EFS:**\n",
      "\n",
      "* To satisfy HIPAA requirements, PHI (Protected Health Information) must be encrypted at-rest on EFS.\n",
      "* Two options are available: enabling encryption at-rest when creating a new file system, which uses AES-256 encryption and AWS KMS-managed keys, or encrypting data before placing it on EFS, which requires customer-managed encryption and key management.\n",
      "* PHI should not be used in file or folder names.\n",
      "* Encryption in transit is provided by Transport Layer Security (TLS) between the EFS service and the instance mounting the file system, which must be enabled using the EFS mount helper or by configuring NFS clients to connect through a TLS tunnel.\n",
      "\n",
      "**Amazon EKS:**\n",
      "\n",
      "* Amazon EKS is a managed service for running Kubernetes on AWS.\n",
      "* For additional security and compliance information, refer to the \"Architecting for HIPAA Security and Compliance on Amazon EKS\" whitepaper.\n",
      "\n",
      "**Amazon ElastiCache for Redis:**\n",
      "\n",
      "* Amazon ElastiCache for Redis is a Redis-compatible in-memory data structure service that can be used as a data store or cache.\n",
      "* To store PHI, customers must ensure they are running the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types, specifically:\n",
      "\t+ Node Types: current generation only (e.g., M4, M5, R4, R5, T2, T3)\n",
      "\t+ ElastiCache for Redis engine version: 3.2.6 and 4.0.10 onwards\n",
      "* Additionally, customers must configure the cluster and nodes to:\n",
      "\t+ Encrypt data at rest using hardware-accelerated AES-512 symmetric keys\n",
      "\t+ Enable transport encryption using TLS\n",
      "\t+ Enable authentication of Redis commands using a Redis AUTH token, which must meet specific constraints (printable ASCII characters, 16-128 characters in length, no specific characters)\n",
      "\t+ Apply the latest 'Security' type service updates on or before the 'Recommended Apply by Date', which can be tracked using the 'SLA Met' field in the service update feature and the service updates history dashboard\n",
      "* Redis backups are encrypted using Amazon S3-managed encryption keys (SSE-S3) with hardware-accelerated AES-256 symmetric keys.\n",
      "* Note that customers should continue to evaluate and determine whether Amazon ElastiCache for Redis encryption satisfies their compliance and regulatory requirements, as the Guidance may be updated.\n",
      "\n",
      "**Amazon OpenSearch Service:**\n",
      "\n",
      "* When using OpenSearch Service with PHI, customers should use OpenSearch or Elasticsearch 6.0 or later.\n",
      "* Customers should ensure PHI is encrypted at-rest and in-transit within Amazon OpenSearch Service.\n",
      "* Customers may use AWS KMS key encryption to encrypt data at rest in their OpenSearch Service domains, which is only available for OpenSearch and Elasticsearch 5.1 or later.\n",
      "* Node-to-node encryption should be enabled, which is available in all OpenSearch versions, and in Elasticsearch 6.0 or later.\n",
      "* PHI should be sent over HTTPS to ensure encryption in transit.\n",
      "\n",
      "**Amazon EMR:**\n",
      "\n",
      "* For information on encryption with Amazon EMR, refer to the \"Encryption Options\" documentation.\n",
      "\n",
      "**Amazon EventBridge:**\n",
      "\n",
      "* Amazon EventBridge is a serverless event bus that enables scalable event-driven applications.\n",
      "* By default, EventBridge encrypts data using 256-bit Advanced Encryption Standard (AES-256) under an AWS owned CMK, which helps secure customer data from unauthorized access.\n",
      "* EventBridge is integrated with AWS CloudTrail, and customers can view the most recent events in the CloudTrail console in Event history.\n",
      "\n",
      "**Amazon Forecast:**\n",
      "\n",
      "* Amazon Forecast is a fully managed service that uses machine learning to deliver highly accurate forecasts.\n",
      "* Every interaction customers have with Amazon Forecast is protected by encryption, and any content processed by Amazon Forecast is encrypted with customer keys through Amazon Key Management Service.\n",
      "* Amazon Forecast is integrated with AWS CloudTrail, which captures all API calls for Amazon Forecast as events.\n",
      "* By default, log files delivered by CloudTrail to an Amazon S3 bucket are encrypted by Amazon server-side encryption with Amazon S3-managed encryption keys (SSE-S3), but customers can use server-side encryption with AWS KMS–managed keys (SSE-KMS) for added security, except for digest files which are encrypted with SSE-S3.\n",
      "* When importing and exporting data from Amazon S3, customers should ensure S3 buckets are configured in a manner consistent with the guidance.\n",
      "\n",
      "**Amazon FSx:**\n",
      "\n",
      "* Amazon FSx is a fully-managed service providing feature-rich and highly-performant file systems.\n",
      "* Amazon FSx for Windows File Server provides highly reliable and scalable file storage and is accessible over the Server Message Block (SMB) protocol.\n",
      "* Amazon FSx for Lustre provides high-performance file storage.\n",
      "* Amazon FSx supports two forms of encryption: encryption of data in transit and encryption at rest.\n",
      "* Encryption of data in transit is supported by Amazon FSx for Windows File Server on compute instances supporting SMB protocol 3.0 or newer, and by Amazon FSx for Lustre on Amazon EC2 instances that support encryption in transit.\n",
      "* Encryption of data at rest is automatically enabled when creating an Amazon FSx file system, using AES-256 encryption algorithm and AWS KMS-managed keys.\n",
      "* PHI should not be used in any file or folder name.\n",
      "\n",
      "**Amazon GuardDuty:**\n",
      "\n",
      "* Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior to help customers protect their AWS accounts and workloads.\n",
      "* It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise.\n",
      "* Amazon GuardDuty also detects potentially compromised instances or reconnaissance by attackers.\n",
      "* It continuously monitors and analyzes VPC Flow Logs, AWS CloudTrail event logs, and DNS logs, using threat intelligence feeds and machine learning to identify unexpected and potentially unauthorized and malicious activity within an AWS environment.\n",
      "* Amazon GuardDuty does not encounter PHI as this data is not to be stored in any of the AWS based data sources listed above.\n",
      "\n",
      "**Amazon HealthLake:**\n",
      "\n",
      "* Amazon HealthLake enables customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale.\n",
      "* Customers can use Amazon HealthLake to transmit, process, and store PHI.\n",
      "* Amazon HealthLake encrypts data at rest in customer’s data stores by default.\n",
      "* All service data and metadata is encrypted with a service-owned KMS key.\n",
      "* Per Fast Healthcare Interoperability Resources (FHIR) specifications, if a customer chooses to use FHIR, Amazon HealthLake will encrypt data at rest and in transit.\n",
      "* When customers delete a FHIR resource, it will only be hidden from retrieval, and will be retained by the service for versioning.\n",
      "* When customers use the StartFHIRImportJob API, Amazon HealthLake will enforce the requirement to export data to an encrypted Amazon S3 bucket.\n",
      "* Amazon HealthLake encrypts data both in transit and at rest.\n",
      "* For encryption of data in transit, clients must support Transport Layer Security (TLS) 1.0 or later, with a recommended version of TLS 1.3.\n",
      "* Clients must also support cipher suites with perfect forward secrecy (PFS) such as Ephemeral Diffie-Hellman (DHE) or Elliptic Curve Ephemeral Diffie-Hellman (ECDHE).\n",
      "* Requests must be signed by using an access key ID and a secret access key that is associated with an IAM principal, or by using the AWS Security Token Service (AWS STS) to generate temporary security credentials to sign requests.\n",
      "* For encryption of data at rest, Amazon HealthLake encrypts data in customer’s data stores with a customer-owned AWS KMS key or by a service-owned AWS KMS key by default.\n",
      "* Amazon HealthLake is integrated with AWS CloudTrail, which captures all API calls to Amazon HealthLake as events, including calls made as a result of interaction with the AWS Management Console, command line interface (CLI), and programmatically using software development kits (SDKs).\n",
      "\n",
      "**Amazon Inspector:**\n",
      "\n",
      "* Amazon Inspector assesses applications for vulnerabilities or deviations from best practices.\n",
      "* After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity.\n",
      "* Customers may run Amazon Inspector on EC2 instances that contain PHI.\n",
      "* Amazon Inspector encrypts all data transmitted over the network as well as all telemetry data stored at-rest.\n",
      "\n",
      "**Amazon Managed Service for Apache Flink:**\n",
      "\n",
      "* Amazon Managed Service for Apache Flink enables customers to quickly author SQL code that continuously reads, processes, and stores data in near real-time.\n",
      "* Using standard SQL queries on the streaming data, customers can construct applications that transform and provide insights into their data.\n",
      "* Managed Service for Apache Flink supports inputs from Kinesis Data Streams and Amazon Inspector.\n",
      "* Firehose delivery streams as sources for analytics applications. If the stream is encrypted, Managed Service for Apache Flink accesses the data in the encrypted stream seamlessly with no further configuration needed.\n",
      "* Managed Service for Apache Flink does not store unencrypted data read from Kinesis Data Streams.\n",
      "* For more information, see Configuring Application Input.\n",
      "* Managed Service for Apache Flink\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent document chunks in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f979c-4d80-408d-9799-20043d9fbe0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully looked at three different summarization techniques using LangChain; **Stuff**, **Map_Reduce**, and **Refine**. Each of these methods has its own distinct advantages/uses. \n",
    "\n",
    "- ***Stuff*** is straighforward and is the fastest method out of the three since it makes a single call to the LLM and fits the entire document within the model's context window. Although as we saw with the HIPAA Compliance document, it does not scale well to work with large volumes of text.\n",
    "\n",
    "- ***Map_Reduce*** deals with the issue of the context window length while being able to parallelize generation of summaries for individual chunks, thereby speeding up the model's response while being able to process long documents. An issue with Map_Reduce is that since this is not a recursive process, we lose context between chunks during this process.\n",
    "\n",
    "- ***Refine*** deals with the issues that arise with the previous methods. It performs recursive summarization by incrementally generating summaries for each of the chunks while retaining context between them. While this method generates the most accurate and comprehensive summary out of all 3 methods, the calls made to the LLM cannot be parallelized. This can result in longer processing times. Additionally, more recent document chunks tend to carry more weight due to the order that they are processed in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Meta\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
